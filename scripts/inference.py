#!/usr/bin/env python3
"""
é˜¿å°”èŒ¨æµ·é»˜ç—‡æ£€æµ‹æ¨¡å‹æ¨ç†è„šæœ¬
Inference script for Alzheimer detection with explanations
"""

import os
import sys
import argparse
import logging
from pathlib import Path
import yaml
import torch
import numpy as np
import json
from typing import Dict, Optional, List
import matplotlib.pyplot as plt
import seaborn as sns

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.models.integrated_model import IntegratedAlzheimerModel
from src.data.audio_processor import AudioProcessor
from src.data.eeg_processor import EEGProcessor
from src.data.text_processor import TextProcessor
from src.utils.visualization import generate_explanation_html

logger = logging.getLogger(__name__)


class AlzheimerPredictor:
    """é˜¿å°”èŒ¨æµ·é»˜ç—‡æ£€æµ‹é¢„æµ‹å™¨"""
    
    def __init__(self, model_path: str, config_path: str):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åŠ è½½é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        # åŠ è½½æ¨¡å‹
        logger.info(f"ä» {model_path} åŠ è½½æ¨¡å‹...")
        checkpoint = torch.load(model_path, map_location=self.device)
        
        self.model = IntegratedAlzheimerModel(checkpoint['config']['model'])
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.to(self.device)
        self.model.eval()
        
        # åˆå§‹åŒ–å¤„ç†å™¨
        self.audio_processor = AudioProcessor(self.config.get('data', {}))
        self.eeg_processor = EEGProcessor(self.config.get('data', {}))
        self.text_processor = TextProcessor(self.config.get('data', {}))
        
        # ç±»åˆ«åç§°
        self.class_names = checkpoint['config']['model']['crf']['class_names']
        
        logger.info("æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def preprocess_inputs(
        self, 
        audio_path: Optional[str] = None,
        eeg_path: Optional[str] = None,
        text: Optional[str] = None
    ) -> Dict[str, torch.Tensor]:
        """é¢„å¤„ç†è¾“å…¥æ•°æ®"""
        inputs = {}
        
        # å¤„ç†éŸ³é¢‘
        if audio_path and Path(audio_path).exists():
            logger.info(f"å¤„ç†éŸ³é¢‘æ–‡ä»¶: {audio_path}")
            audio_features, speech_features = self.audio_processor.process_audio_file(audio_path)
            
            # è½¬æ¢ä¸ºå¼ é‡
            inputs['audio_features'] = torch.tensor(audio_features).unsqueeze(0).to(self.device)
            inputs['speech_features'] = torch.tensor(speech_features).unsqueeze(0).to(self.device)
            
            # å¦‚æœæ²¡æœ‰æä¾›æ–‡æœ¬ï¼Œå°è¯•ä»éŸ³é¢‘æå–
            if not text:
                # è¿™é‡Œå¯ä»¥é›†æˆASRæ¥æå–æ–‡æœ¬
                text = "extracted from audio"  # å ä½ç¬¦
        
        # å¤„ç†EEG
        if eeg_path and Path(eeg_path).exists():
            logger.info(f"å¤„ç†EEGæ–‡ä»¶: {eeg_path}")
            eeg_features = self.eeg_processor.process_eeg_file(eeg_path)
            inputs['eeg_features'] = torch.tensor(eeg_features).unsqueeze(0).to(self.device)
        
        # å¤„ç†æ–‡æœ¬
        if text:
            logger.info("å¤„ç†æ–‡æœ¬è¾“å…¥")
            text_features = self.text_processor.extract_features(text)
            inputs['text_features'] = torch.tensor(text_features).unsqueeze(0).to(self.device)
        
        # å¦‚æœæ²¡æœ‰ä»»ä½•è¾“å…¥ï¼Œåˆ›å»ºè™šæ‹Ÿè¾“å…¥
        if not inputs:
            logger.warning("æœªæä¾›æœ‰æ•ˆè¾“å…¥ï¼Œä½¿ç”¨è™šæ‹Ÿæ•°æ®")
            batch_size = 1
            inputs = {
                'text_features': torch.zeros(batch_size, 768, device=self.device),
                'eeg_features': torch.zeros(batch_size, 100, 19, 15, device=self.device)
            }
        
        return inputs
    
    def predict(
        self,
        audio_path: Optional[str] = None,
        eeg_path: Optional[str] = None,
        text: Optional[str] = None,
        return_explanations: bool = True
    ) -> Dict[str, any]:
        """è¿›è¡Œé¢„æµ‹"""
        
        # é¢„å¤„ç†è¾“å…¥
        inputs = self.preprocess_inputs(audio_path, eeg_path, text)
        
        # æ¨¡å‹æ¨ç†
        with torch.no_grad():
            outputs = self.model(
                inputs, 
                return_concepts=True, 
                return_explanations=return_explanations
            )
        
        # å¤„ç†è¾“å‡º
        diagnosis_probs = outputs['diagnosis_probs'].cpu().numpy()[0]
        predicted_class = int(outputs['diagnosis_predictions'].cpu().numpy()[0])
        predicted_label = self.class_names[predicted_class]
        
        # æ¦‚å¿µé¢„æµ‹
        concept_predictions = {}
        for concept_name, concept_values in outputs['concepts'].items():
            concept_predictions[concept_name] = float(concept_values.cpu().numpy()[0])
        
        results = {
            'prediction': {
                'class_index': predicted_class,
                'class_name': predicted_label,
                'confidence': float(diagnosis_probs[predicted_class]),
                'probabilities': {
                    name: float(prob) for name, prob in zip(self.class_names, diagnosis_probs)
                }
            },
            'concepts': concept_predictions,
            'input_info': {
                'has_audio': audio_path is not None,
                'has_eeg': eeg_path is not None,
                'has_text': text is not None,
                'audio_path': audio_path,
                'eeg_path': eeg_path,
                'text_length': len(text) if text else 0
            }
        }
        
        # æ·»åŠ è§£é‡Š (å¦‚æœéœ€è¦)
        if return_explanations and 'explanations' in outputs:
            results['explanations'] = outputs['explanations']
        
        return results
    
    def predict_batch(
        self,
        data_list: List[Dict[str, str]],
        return_explanations: bool = False
    ) -> List[Dict[str, any]]:
        """æ‰¹é‡é¢„æµ‹"""
        results = []
        
        for data_item in data_list:
            try:
                result = self.predict(
                    audio_path=data_item.get('audio_path'),
                    eeg_path=data_item.get('eeg_path'),
                    text=data_item.get('text'),
                    return_explanations=return_explanations
                )
                result['item_id'] = data_item.get('id', len(results))
                results.append(result)
            except Exception as e:
                logger.error(f"å¤„ç†é¡¹ç›® {data_item.get('id', len(results))} æ—¶å‡ºé”™: {e}")
                results.append({
                    'item_id': data_item.get('id', len(results)),
                    'error': str(e)
                })
        
        return results
    
    def generate_report(
        self,
        results: Dict[str, any],
        output_path: str,
        include_plots: bool = True
    ):
        """ç”Ÿæˆè¯¦ç»†çš„é¢„æµ‹æŠ¥å‘Š"""
        
        # åˆ›å»ºHTMLæŠ¥å‘Š
        html_content = self._create_html_report(results, include_plots)
        
        # ä¿å­˜HTMLæ–‡ä»¶
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        logger.info(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
    
    def _create_html_report(self, results: Dict[str, any], include_plots: bool) -> str:
        """åˆ›å»ºHTMLæŠ¥å‘Šå†…å®¹"""
        
        prediction = results['prediction']
        concepts = results['concepts']
        input_info = results['input_info']
        
        html_content = f"""
        <!DOCTYPE html>
        <html lang="zh-CN">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>é˜¿å°”èŒ¨æµ·é»˜ç—‡æ£€æµ‹æŠ¥å‘Š</title>
            <style>
                body {{ font-family: 'Microsoft YaHei', Arial, sans-serif; margin: 20px; }}
                .header {{ background-color: #f0f8ff; padding: 20px; border-radius: 10px; }}
                .result {{ background-color: #f9f9f9; padding: 15px; margin: 10px 0; border-radius: 5px; }}
                .concept {{ background-color: #fff; padding: 10px; margin: 5px 0; border-left: 4px solid #007acc; }}
                .confidence {{ font-size: 24px; font-weight: bold; color: #d9534f; }}
                .concept-value {{ font-weight: bold; color: #5bc0de; }}
                .info {{ color: #666; font-size: 14px; }}
                .chart {{ margin: 20px 0; text-align: center; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>ğŸ§  é˜¿å°”èŒ¨æµ·é»˜ç—‡å¤šæ¨¡æ€æ£€æµ‹æŠ¥å‘Š</h1>
                <p class="info">åŸºäºæ¦‚å¿µç“¶é¢ˆæ¨¡å‹çš„å¯è§£é‡Šæ€§è¯Šæ–­ç³»ç»Ÿ</p>
            </div>
            
            <div class="result">
                <h2>ğŸ“Š è¯Šæ–­ç»“æœ</h2>
                <p><strong>é¢„æµ‹åˆ†ç±»:</strong> <span style="color: #d9534f; font-size: 20px;">{prediction['class_name']}</span></p>
                <p><strong>ç½®ä¿¡åº¦:</strong> <span class="confidence">{prediction['confidence']:.2%}</span></p>
                
                <h3>å„ç±»åˆ«æ¦‚ç‡:</h3>
                <ul>
        """
        
        for class_name, prob in prediction['probabilities'].items():
            html_content += f"<li><strong>{class_name}:</strong> {prob:.2%}</li>"
        
        html_content += """
                </ul>
            </div>
            
            <div class="result">
                <h2>ğŸ” åŒ»å­¦æ¦‚å¿µåˆ†æ</h2>
                <p class="info">ä»¥ä¸‹æ˜¯ä»å¤šæ¨¡æ€æ•°æ®ä¸­æå–çš„åŒ»å­¦ç›¸å…³æ¦‚å¿µï¼Œä¸ºè¯Šæ–­æä¾›å¯è§£é‡Šæ€§æ”¯æŒï¼š</p>
        """
        
        # æ¦‚å¿µè§£é‡Šæ˜ å°„
        concept_descriptions = {
            'speech_rate': 'è¯­é€Ÿ - æ¯ç§’è¯´è¯çš„è¯æ•°ï¼Œåæ˜ è¯­è¨€æµç•…æ€§',
            'pause_ratio': 'åœé¡¿æ¯”ä¾‹ - è¯­éŸ³ä¸­åœé¡¿æ—¶é—´çš„å æ¯”ï¼Œåæ˜ è¯­è¨€è§„åˆ’èƒ½åŠ›',
            'lexical_richness': 'è¯æ±‡ä¸°å¯Œåº¦ - è¯æ±‡å¤šæ ·æ€§æŒ‡æ ‡ï¼Œåæ˜ è¯­è¨€è¡¨è¾¾èƒ½åŠ›',
            'syntactic_complexity': 'å¥æ³•å¤æ‚åº¦ - å¥å­ç»“æ„çš„å¤æ‚ç¨‹åº¦ï¼Œåæ˜ è®¤çŸ¥åŠŸèƒ½',
            'alpha_power': 'Alphaæ³¢åŠŸç‡ - 8-13Hzè„‘ç”µæ³¢åŠŸç‡ï¼Œåæ˜ æ³¨æ„åŠ›å’Œè®¤çŸ¥çŠ¶æ€',
            'theta_beta_ratio': 'Theta/Betaæ¯”å€¼ - è„‘ç”µæ³¢æ¯”å€¼ï¼Œåæ˜ è®¤çŸ¥æ§åˆ¶èƒ½åŠ›',
            'gamma_connectivity': 'Gammaè¿é€šæ€§ - é«˜é¢‘è„‘ç”µè¿é€šæ€§ï¼Œåæ˜ ä¿¡æ¯æ•´åˆèƒ½åŠ›'
        }
        
        for concept_name, value in concepts.items():
            description = concept_descriptions.get(concept_name, concept_name)
            html_content += f"""
                <div class="concept">
                    <p><strong>{description}</strong></p>
                    <p>å€¼: <span class="concept-value">{value:.3f}</span></p>
                </div>
            """
        
        html_content += f"""
            </div>
            
            <div class="result">
                <h2>ğŸ“‹ è¾“å…¥ä¿¡æ¯</h2>
                <ul>
                    <li><strong>éŸ³é¢‘æ–‡ä»¶:</strong> {'âœ“ å·²æä¾›' if input_info['has_audio'] else 'âœ— æœªæä¾›'}</li>
                    <li><strong>EEGæ–‡ä»¶:</strong> {'âœ“ å·²æä¾›' if input_info['has_eeg'] else 'âœ— æœªæä¾›'}</li>
                    <li><strong>æ–‡æœ¬è¾“å…¥:</strong> {'âœ“ å·²æä¾›' if input_info['has_text'] else 'âœ— æœªæä¾›'}</li>
                </ul>
                
                {f"<p><strong>éŸ³é¢‘è·¯å¾„:</strong> {input_info['audio_path']}</p>" if input_info['audio_path'] else ""}
                {f"<p><strong>EEGè·¯å¾„:</strong> {input_info['eeg_path']}</p>" if input_info['eeg_path'] else ""}
                {f"<p><strong>æ–‡æœ¬é•¿åº¦:</strong> {input_info['text_length']} å­—ç¬¦</p>" if input_info['text_length'] > 0 else ""}
            </div>
            
            <div class="result">
                <h2>âš ï¸ é‡è¦è¯´æ˜</h2>
                <p class="info">
                    æ­¤æŠ¥å‘Šä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç”Ÿçš„è¯Šæ–­ã€‚å¦‚æœ‰ç–‘è™‘ï¼Œè¯·å’¨è¯¢ä¸“ä¸šåŒ»ç–—äººå‘˜ã€‚
                    æœ¬ç³»ç»ŸåŸºäºäººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œå¯èƒ½å­˜åœ¨è¯¯å·®ï¼Œè¯·ç»“åˆä¸´åºŠè¡¨ç°ç»¼åˆåˆ¤æ–­ã€‚
                </p>
            </div>
            
            <div class="info" style="text-align: center; margin-top: 30px;">
                <p>æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
                <p>ç³»ç»Ÿç‰ˆæœ¬: é˜¿å°”èŒ¨æµ·é»˜ç—‡å¤šæ¨¡æ€æ£€æµ‹ç³»ç»Ÿ v1.0</p>
            </div>
        </body>
        </html>
        """
        
        return html_content


def main():
    parser = argparse.ArgumentParser(description="é˜¿å°”èŒ¨æµ·é»˜ç—‡æ£€æµ‹æ¨¡å‹æ¨ç†")
    parser.add_argument('--model_path', type=str, required=True, help='æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--config_path', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--audio_path', type=str, help='éŸ³é¢‘æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--eeg_path', type=str, help='EEGæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--text', type=str, help='æ–‡æœ¬è¾“å…¥')
    parser.add_argument('--output_json', type=str, help='JSONç»“æœè¾“å‡ºè·¯å¾„')
    parser.add_argument('--output_html', type=str, help='HTMLæŠ¥å‘Šè¾“å‡ºè·¯å¾„')
    parser.add_argument('--batch_file', type=str, help='æ‰¹é‡å¤„ç†çš„JSONæ–‡ä»¶')
    parser.add_argument('--debug', action='store_true', help='è°ƒè¯•æ¨¡å¼')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.DEBUG if args.debug else logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # ç¡®å®šé…ç½®æ–‡ä»¶è·¯å¾„
    if args.config_path:
        config_path = args.config_path
    else:
        # å°è¯•ä»æ¨¡å‹ç›®å½•æ‰¾é…ç½®æ–‡ä»¶
        model_dir = Path(args.model_path).parent.parent
        config_path = model_dir / "config" / "model_config.yaml"
        if not config_path.exists():
            config_path = "config/model_config.yaml"
    
    # åˆ›å»ºé¢„æµ‹å™¨
    predictor = AlzheimerPredictor(args.model_path, config_path)
    
    if args.batch_file:
        # æ‰¹é‡å¤„ç†
        logger.info(f"æ‰¹é‡å¤„ç†æ–‡ä»¶: {args.batch_file}")
        with open(args.batch_file, 'r', encoding='utf-8') as f:
            data_list = json.load(f)
        
        results = predictor.predict_batch(data_list, return_explanations=True)
        
        # ä¿å­˜æ‰¹é‡ç»“æœ
        if args.output_json:
            with open(args.output_json, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            logger.info(f"æ‰¹é‡ç»“æœå·²ä¿å­˜åˆ°: {args.output_json}")
    
    else:
        # å•ä¸ªé¢„æµ‹
        logger.info("å¼€å§‹å•ä¸ªé¢„æµ‹...")
        results = predictor.predict(
            audio_path=args.audio_path,
            eeg_path=args.eeg_path,
            text=args.text,
            return_explanations=True
        )
        
        # è¾“å‡ºç»“æœ
        print("\n" + "="*50)
        print("é¢„æµ‹ç»“æœ:")
        print(f"åˆ†ç±»: {results['prediction']['class_name']}")
        print(f"ç½®ä¿¡åº¦: {results['prediction']['confidence']:.2%}")
        print("\næ¦‚å¿µé¢„æµ‹:")
        for concept, value in results['concepts'].items():
            print(f"  {concept}: {value:.3f}")
        print("="*50)
        
        # ä¿å­˜JSONç»“æœ
        if args.output_json:
            with open(args.output_json, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            logger.info(f"JSONç»“æœå·²ä¿å­˜åˆ°: {args.output_json}")
        
        # ç”ŸæˆHTMLæŠ¥å‘Š
        if args.output_html:
            predictor.generate_report(results, args.output_html)


if __name__ == "__main__":
    import pandas as pd  # ç”¨äºæ—¶é—´æˆ³
    main() 