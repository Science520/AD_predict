#!/usr/bin/env python3
"""
7ç»´åº¦å£°å­¦ç‰¹å¾æå–å™¨ - åŸºäºè¡¨æ ¼è®¾è®¡çš„å®Œæ•´å®ç°
æŒ‰ç…§MCIæ£€æµ‹çš„7ä¸ªå£°å­¦ç‰¹å¾ç»´åº¦è¿›è¡ŒéŸ³é¢‘åˆ†æ
"""
import sys
import logging
from pathlib import Path
import whisper
import librosa
import numpy as np
import re
from typing import Dict, List, Tuple
import scipy.stats
from scipy.signal import find_peaks
# ç§»é™¤parselmouthä¾èµ–ï¼Œä½¿ç”¨librosaæ›¿ä»£

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SevenDimensionAcousticExtractor:
    """7ç»´åº¦å£°å­¦ç‰¹å¾æå–å™¨ - ä½¿ç”¨librosaå®ç°"""
    
    def __init__(self, model_size="base"):
        self.model = whisper.load_model(model_size)
        self.sample_rate = 16000
        
    def extract_prosody_features(self, audio_path: str) -> Dict:
        """1. éŸµå¾‹å’Œè¯­è°ƒ (Prosody) ç‰¹å¾æå– - ä½¿ç”¨librosa"""
        
        # åŠ è½½éŸ³é¢‘
        audio, sr = librosa.load(audio_path, sr=self.sample_rate)
        
        # æå–åŸºé¢‘(F0) - ä½¿ç”¨librosaçš„piptrack
        pitches, magnitudes = librosa.piptrack(y=audio, sr=sr, threshold=0.1)
        
        # æå–æœ‰æ•ˆçš„F0å€¼
        f0_values = []
        for t in range(pitches.shape[1]):
            index = magnitudes[:, t].argmax()
            pitch = pitches[index, t]
            if pitch > 0:
                f0_values.append(pitch)
        
        if len(f0_values) == 0:
            return {'f0_std': 0, 'f0_range': 0, 'f0_mean': 0, 'pitch_range_st': 0, 'prosody_score': 0}
        
        f0_values = np.array(f0_values)
        
        # è®¡ç®—éŸµå¾‹ç‰¹å¾
        f0_std = np.std(f0_values)
        f0_range = np.max(f0_values) - np.min(f0_values)
        f0_mean = np.mean(f0_values)
        
        # è®¡ç®—éŸ³é«˜èŒƒå›´ï¼ˆåŠéŸ³ï¼‰
        pitch_range_st = 12 * np.log2(np.max(f0_values) / np.min(f0_values)) if np.min(f0_values) > 0 else 0
        
        # è¯­è°ƒè½®å»“åˆ†æ
        f0_slopes = np.diff(f0_values)
        slope_variability = np.std(f0_slopes) if len(f0_slopes) > 0 else 0
        
        return {
            'f0_std': float(f0_std),
            'f0_range': float(f0_range), 
            'f0_mean': float(f0_mean),
            'pitch_range_st': float(pitch_range_st),
            'slope_variability': float(slope_variability),
            'prosody_score': 0  # å¾…äººå·¥æ ‡æ³¨è®­ç»ƒ
        }
    
    def extract_voice_quality_features(self, audio_path: str) -> Dict:
        """2. éŸ³è´¨å’Œç¨³å®šæ€§ (Voice Quality) ç‰¹å¾æå– - ä½¿ç”¨librosa"""
        
        # åŠ è½½éŸ³é¢‘
        audio, sr = librosa.load(audio_path, sr=self.sample_rate)
        
        # ç®€åŒ–çš„jitter/shimmerè®¡ç®—
        # æå–åŸºé¢‘è¿›è¡Œå‘¨æœŸæ€§åˆ†æ
        pitches, magnitudes = librosa.piptrack(y=audio, sr=sr, threshold=0.1)
        
        # æå–æœ‰æ•ˆçš„F0å€¼ç”¨äºjitterè®¡ç®—
        f0_times = []
        for t in range(pitches.shape[1]):
            index = magnitudes[:, t].argmax()
            pitch = pitches[index, t]
            if pitch > 0:
                f0_times.append(pitch)
        
        # ç®€åŒ–çš„jitterè®¡ç®—ï¼ˆåŸºäºF0å˜åŒ–ï¼‰
        if len(f0_times) > 1:
            f0_diffs = np.abs(np.diff(f0_times))
            jitter = np.mean(f0_diffs) / np.mean(f0_times) * 100 if np.mean(f0_times) > 0 else 0
        else:
            jitter = 0
        
        # ç®€åŒ–çš„shimmerè®¡ç®—ï¼ˆåŸºäºå¹…åº¦å˜åŒ–ï¼‰
        rms_energy = librosa.feature.rms(y=audio, frame_length=512, hop_length=256)[0]
        if len(rms_energy) > 1:
            amplitude_diffs = np.abs(np.diff(rms_energy))
            shimmer = np.mean(amplitude_diffs) / np.mean(rms_energy) if np.mean(rms_energy) > 0 else 0
        else:
            shimmer = 0
        
        # ç®€åŒ–çš„HNRè®¡ç®—ï¼ˆä½¿ç”¨é¢‘è°±ç‰¹å¾ï¼‰
        spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sr)[0]
        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)[0]
        
        # ä½¿ç”¨é¢‘è°±ç‰¹å¾ä½œä¸ºHNRçš„ä»£ç†æŒ‡æ ‡
        hnr_proxy = np.mean(spectral_centroid / spectral_rolloff) * 20 if np.mean(spectral_rolloff) > 0 else 0
        
        return {
            'jitter': float(jitter),
            'shimmer': float(shimmer), 
            'hnr': float(hnr_proxy),
            'voice_quality_score': 0  # å¾…äººå·¥æ ‡æ³¨è®­ç»ƒ
        }
    
    def extract_articulation_features(self, audio_path: str) -> Dict:
        """3. å‘éŸ³æ¸…æ™°åº¦ (Articulation) ç‰¹å¾æå– - ä½¿ç”¨librosa"""
        
        # åŠ è½½éŸ³é¢‘è¿›è¡Œå…±æŒ¯å³°åˆ†æ
        audio, sr = librosa.load(audio_path, sr=self.sample_rate)
        
        # ä½¿ç”¨MFCCä½œä¸ºå‘éŸ³æ¸…æ™°åº¦çš„ä»£ç†æŒ‡æ ‡
        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)
        
        # è®¡ç®—MFCCçš„å˜å¼‚æ€§ä½œä¸ºå‘éŸ³æ¸…æ™°åº¦æŒ‡æ ‡
        mfcc_variability = np.mean(np.std(mfccs, axis=1))
        
        # ä½¿ç”¨é¢‘è°±è´¨å¿ƒå’Œå¸¦å®½ä½œä¸ºå…±æŒ¯å³°çš„ä»£ç†
        spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr)[0]
        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sr)[0]
        
        # è®¡ç®—é¢‘è°±å˜åŒ–ç‡ä½œä¸ºå…±æŒ¯å³°å˜åŒ–çš„ä»£ç†
        centroid_changes = np.abs(np.diff(spectral_centroids))
        formant_transition_rate = np.mean(centroid_changes) if len(centroid_changes) > 0 else 0
        
        # ä½¿ç”¨é¢‘è°±ç‰¹å¾ä½œä¸ºVSAçš„ä»£ç†æŒ‡æ ‡
        vsa_proxy = np.std(spectral_centroids) * np.std(spectral_bandwidth)
        
        return {
            'vsa_proxy': float(vsa_proxy),
            'formant_transition_rate': float(formant_transition_rate),
            'mfcc_variability': float(mfcc_variability),
            'spectral_clarity': float(np.mean(spectral_centroids)),
            'articulation_clarity_score': 0  # å¾…äººå·¥æ ‡æ³¨è®­ç»ƒ
        }
    
    def extract_rhythm_features(self, audio_path: str, segments: List[Dict]) -> Dict:
        """4. è¯­é€Ÿå’ŒèŠ‚å¾‹ (Speech Rate & Rhythm) ç‰¹å¾æå–"""
        
        if not segments:
            return {'articulation_rate': 0, 'syllable_variability': 0}
        
        # è®¡ç®—éŸ³èŠ‚æ—¶é•¿
        syllable_durations = []
        for segment in segments:
            duration = segment['end'] - segment['start']
            text = segment['text'].strip()
            # ç®€åŒ–çš„ä¸­æ–‡éŸ³èŠ‚è®¡æ•°
            syllable_count = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
            if syllable_count > 0:
                syllable_durations.append(duration / syllable_count)
        
        if syllable_durations:
            articulation_rate = 1.0 / np.mean(syllable_durations)  # éŸ³èŠ‚/ç§’
            syllable_variability = np.std(syllable_durations)
        else:
            articulation_rate = 0
            syllable_variability = 0
        
        return {
            'articulation_rate': articulation_rate,
            'syllable_variability': syllable_variability,
            'speech_rate_chars_per_min': 0,  # å°†åœ¨åé¢è®¡ç®—
            'rhythm_anomalies': []  # å¾…äººå·¥æ ‡æ³¨è®­ç»ƒ
        }
    
    def extract_transcription_features(self, text: str, segments: List[Dict]) -> Dict:
        """5. æ–‡æœ¬å†…å®¹ä¸å¡«å……è¯ (Transcription & Filled Pauses) ç‰¹å¾æå–"""
        
        # æ£€æµ‹å¡«å……è¯
        filled_pauses = ['å—¯', 'å•Š', 'å‘ƒ', 'å‘', 'é‚£ä¸ª', 'å°±æ˜¯']
        filled_pause_count = 0
        
        for fp in filled_pauses:
            filled_pause_count += text.count(fp)
        
        # æ£€æµ‹é‡å¤
        words = text.split()
        word_counts = {}
        for word in words:
            if len(word) > 1:
                word_counts[word] = word_counts.get(word, 0) + 1
        
        repetitions = sum(1 for count in word_counts.values() if count > 1)
        
        # è‡ªæˆ‘çº æ­£æ£€æµ‹ï¼ˆç®€å•ç‰ˆï¼‰
        self_corrections = text.count('ä¸å¯¹') + text.count('ä¸æ˜¯') + text.count('åº”è¯¥æ˜¯')
        
        return {
            'filled_pauses_count': filled_pause_count,
            'repetitions': repetitions,
            'self_corrections': self_corrections,
            'total_chars': len([c for c in text if c.strip()]),
            'filled_pause_ratio': filled_pause_count / len(words) if words else 0
        }
    
    def analyze_pause_functions(self, pauses: List[Dict], segments: List[Dict]) -> Dict:
        """6. åœé¡¿ç»“æ„ä¸åŠŸèƒ½ (Pause Structure & Function) - å¢å¼ºç‰ˆ"""
        
        if not pauses:
            return {
                'functional_pauses': {'syntactic': 0, 'hesitation': 0, 'word_finding': 0, 'breathing': 0},
                'pause_position_analysis': {}
            }
        
        # ç®€åŒ–çš„åœé¡¿åŠŸèƒ½åˆ†ç±»ï¼ˆåŸºäºæ—¶é•¿å’Œä½ç½®ï¼‰
        functional_pauses = {'syntactic': 0, 'hesitation': 0, 'word_finding': 0, 'breathing': 0}
        
        for pause in pauses:
            duration = pause['duration']
            
            # åŸºäºæ—¶é•¿çš„åˆæ­¥åˆ†ç±»
            if duration < 0.5:
                functional_pauses['syntactic'] += 1
            elif duration < 1.0:
                functional_pauses['hesitation'] += 1
            elif duration < 2.0:
                functional_pauses['word_finding'] += 1
            else:
                functional_pauses['breathing'] += 1
        
        # åœé¡¿ä½ç½®åˆ†æ
        mid_sentence_pauses = 0
        sentence_boundary_pauses = 0
        
        # è¿™é‡Œéœ€è¦æ›´å¤æ‚çš„è¯­æ³•åˆ†æï¼Œæš‚æ—¶ç®€åŒ–å¤„ç†
        for pause in pauses:
            # å‡è®¾åˆ¤æ–­é€»è¾‘
            if pause['duration'] > 1.0:
                mid_sentence_pauses += 1
            else:
                sentence_boundary_pauses += 1
        
        return {
            'functional_pauses': functional_pauses,
            'pause_position_analysis': {
                'mid_sentence': mid_sentence_pauses,
                'sentence_boundary': sentence_boundary_pauses
            }
        }
    
    def detect_articulatory_errors(self, audio_path: str, segments: List[Dict]) -> Dict:
        """7. æ„éŸ³éšœç¢ç‚¹ (Articulatory Error Points) æ£€æµ‹"""
        
        # è¿™æ˜¯ä¸€ä¸ªå¤æ‚çš„ä»»åŠ¡ï¼Œéœ€è¦ä¸“é—¨çš„æ¨¡å‹
        # è¿™é‡Œæä¾›åŸºç¡€æ¡†æ¶ï¼Œå®é™…å®ç°éœ€è¦è®­ç»ƒä¸“é—¨çš„é”™è¯¯æ£€æµ‹æ¨¡å‹
        
        articulatory_errors = []
        error_confidence_scores = []
        
        # åŸºäºASRç½®ä¿¡åº¦çš„åˆæ­¥é”™è¯¯æ£€æµ‹
        for segment in segments:
            if 'confidence' in segment:
                confidence = segment.get('confidence', 1.0)
                if confidence < 0.7:  # ä½ç½®ä¿¡åº¦å¯èƒ½è¡¨ç¤ºå‘éŸ³é—®é¢˜
                    articulatory_errors.append({
                        'text': segment['text'],
                        'start': segment['start'],
                        'end': segment['end'],
                        'error_type': 'LOW_CONFIDENCE',
                        'confidence': confidence
                    })
        
        return {
            'error_points': articulatory_errors,
            'error_types': {'SLUR': 0, 'SUB': 0, 'OMIT': 0},  # å¾…è®­ç»ƒä¸“é—¨æ¨¡å‹
            'total_errors': len(articulatory_errors)
        }
    
    def detect_pauses(self, audio_path: str, segments: List[Dict]) -> Dict:
        """åœé¡¿æ£€æµ‹ - ä¿æŒåŸæœ‰åŠŸèƒ½"""
        
        # åŠ è½½éŸ³é¢‘
        audio, sr = librosa.load(audio_path, sr=self.sample_rate)
        audio_duration = len(audio) / sr
        
        # è®¡ç®—éŸ³é¢‘èƒ½é‡
        frame_length = int(0.025 * sr)  # 25ms
        hop_length = int(0.01 * sr)     # 10ms
        
        energy = librosa.feature.rms(
            y=audio,
            frame_length=frame_length,
            hop_length=hop_length
        )[0]
        
        # æ£€æµ‹é™éŸ³æ®µ
        silence_threshold = np.mean(energy) * 0.1
        silence_mask = energy < silence_threshold
        
        # è½¬æ¢ä¸ºæ—¶é—´æˆ³
        time_frames = librosa.frames_to_time(
            np.arange(len(energy)),
            sr=sr,
            hop_length=hop_length
        )
        
        # æ‰¾åˆ°åœé¡¿æ®µè½
        pauses = []
        in_pause = False
        pause_start = 0
        min_pause_duration = 0.3  # æœ€å°åœé¡¿æ—¶é•¿
        
        for i, is_silent in enumerate(silence_mask):
            if is_silent and not in_pause:
                pause_start = time_frames[i]
                in_pause = True
            elif not is_silent and in_pause:
                pause_duration = time_frames[i] - pause_start
                if pause_duration >= min_pause_duration:
                    pauses.append({
                        'start': pause_start,
                        'end': time_frames[i],
                        'duration': pause_duration
                    })
                in_pause = False
        
        # å¢å¼ºçš„åœé¡¿åˆ†æ
        pause_analysis = self.analyze_pause_functions(pauses, segments)
        
        # è®¡ç®—åœé¡¿ç»Ÿè®¡
        total_pause_time = sum(p['duration'] for p in pauses)
        pause_ratio = total_pause_time / audio_duration if audio_duration > 0 else 0
        
        return {
            'pauses': pauses,
            'total_pause_time': total_pause_time,
            'pause_ratio': pause_ratio,
            'pause_count': len(pauses),
            'audio_duration': audio_duration,
            'average_pause_duration': total_pause_time / len(pauses) if pauses else 0,
            'functional_analysis': pause_analysis
        }
    
    def process_audio_7_dimensions(self, audio_path: str) -> Dict:
        """æŒ‰ç…§7ä¸ªç»´åº¦å®Œæ•´å¤„ç†éŸ³é¢‘"""
        
        logger.info(f"ğŸ¯ å¼€å§‹7ç»´åº¦å£°å­¦ç‰¹å¾æå–: {Path(audio_path).name}")
        
        # 1. ASRè½¬å½• (åŸºç¡€)
        result = self.model.transcribe(
            audio_path,
            language='zh',
            word_timestamps=True,
            verbose=False
        )
        
        text = result['text']
        segments = result.get('segments', [])
        
        logger.info("âœ… å®ŒæˆASRè½¬å½•")
        
        # 2. åœé¡¿æ£€æµ‹
        pause_info = self.detect_pauses(audio_path, segments)
        
        # 3. æ„å»ºå¸¦æ ‡è®°çš„æ–‡æœ¬
        marked_text = self.create_marked_text(text, segments, pause_info)
        
        # 4. æŒ‰segment/wordçº§åˆ«æå–ç‰¹å¾
        acoustic_feature_map = {}
        
        try:
            # ä¸ºæ¯ä¸ªsegmentæå–å¥å­çº§åˆ«ç‰¹å¾
            for i, segment in enumerate(segments):
                segment_id = f"segment_{i}"
                
                # æå–è¯¥segmentçš„å£°å­¦ç‰¹å¾
                segment_features = self.extract_segment_features(audio_path, segment)
                acoustic_feature_map[segment_id] = segment_features
                
            # è¯çº§åˆ«ç‰¹å¾ï¼ˆå‘éŸ³æ¸…æ™°åº¦ç›¸å…³ï¼‰
            word_features = self.extract_word_level_features(audio_path, segments)
            acoustic_feature_map.update(word_features)
            
            # å…¨å±€åœé¡¿åˆ†æ
            acoustic_feature_map['global_pause_analysis'] = {
                'pause_structure': pause_info['functional_analysis'],
                'pause_statistics': {
                    'total_count': pause_info['pause_count'],
                    'total_duration': pause_info['total_pause_time'],
                    'pause_rate': pause_info['pause_ratio'],
                    'average_duration': pause_info['average_pause_duration']
                },
                'pause_details': self.format_pause_details(pause_info['pauses'])
            }
            
            logger.info("âœ… å®Œæˆæ‰€æœ‰ç‰¹å¾æå–")
            
        except Exception as e:
            logger.error(f"âŒ ç‰¹å¾æå–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return None
        
        # 5. æ„å»ºç¬¦åˆæ–‡æ¡£è®¾è®¡çš„è¾“å‡ºç»“æ„
        enhanced_output = {
            # åŸºç¡€ä¿¡æ¯
            'audio_file': str(audio_path),
            'text': marked_text,  # å¸¦æ ‡è®°çš„æ–‡æœ¬
            'original_text': text,
            'segments': segments,
            
            # æŒ‰æ–‡æ¡£è®¾è®¡çš„ç‰¹å¾æ˜ å°„ç»“æ„
            'acoustic_feature_map': acoustic_feature_map,
            
            # å¾…æ ‡æ³¨çš„æ„ŸçŸ¥è¯„ä¼°åˆ†æ•° (äººå·¥æ ‡æ³¨ç›®æ ‡)
            'perceptual_scores': {
                'prosody_flatness': None,      # 1-4åˆ†
                'voice_hoarseness': None,      # 1-4åˆ†  
                'articulation_clarity': None,  # 1-3åˆ†
                'rhythm_anomalies': None,      # æ ‡è®°å¼‚å¸¸ç‰‡æ®µ
                'pause_functions': None        # åœé¡¿åŠŸèƒ½åˆ†ç±»
            },
            
            # æ€»ä½“è¯„ä¼°
            'summary': {
                'total_duration': pause_info['audio_duration'],
                'speech_rate': len([c for c in text if c.strip()]) / pause_info['audio_duration'] * 60 if pause_info['audio_duration'] > 0 else 0,
                'pause_ratio': pause_info['pause_ratio'],
                'feature_extraction_success': True
            }
        }
        
        return enhanced_output
    
    def create_marked_text(self, text: str, segments: List[Dict], pause_info: Dict) -> str:
        """åˆ›å»ºå¸¦åœé¡¿å’Œé”™è¯¯æ ‡è®°çš„æ–‡æœ¬"""
        
        if not segments:
            return text
            
        pauses = pause_info['pauses']
        result_text = ""
        last_end = 0
        
        for segment in segments:
            seg_start = segment['start']
            seg_text = segment['text'].strip()
            
            # æ£€æŸ¥æ˜¯å¦åœ¨æ­¤åˆ†æ®µå‰æœ‰åœé¡¿
            for pause in pauses:
                if last_end <= pause['start'] <= seg_start:
                    if pause['duration'] >= 0.5:  # åªæ ‡è®°è¾ƒé•¿çš„åœé¡¿
                        pause_func = self.classify_pause_function(pause['duration'])
                        result_text += f"<pause dur=\"{pause['duration']:.1f}s\" func=\"{pause_func}\">"
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å‘éŸ³é”™è¯¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
            if 'confidence' in segment and segment.get('confidence', 1.0) < 0.7:
                result_text += f"<error type=\"LOW_CONF\" confidence=\"{segment.get('confidence', 0):.2f}\">{seg_text}</error>"
            else:
                result_text += seg_text
                
            last_end = segment['end']
            
        return result_text.strip()
    
    def classify_pause_function(self, duration: float) -> str:
        """æ ¹æ®æ—¶é•¿åˆæ­¥åˆ†ç±»åœé¡¿åŠŸèƒ½"""
        if duration < 0.5:
            return "syntactic"
        elif duration < 1.0:
            return "hesitation"
        elif duration < 2.0:
            return "word_finding"
        else:
            return "breathing"
    
    def extract_segment_features(self, audio_path: str, segment: Dict) -> Dict:
        """æå–å•ä¸ªsegmentçš„å¥å­çº§åˆ«ç‰¹å¾"""
        
        # å¯¹äºæ¯ä¸ªsegmentï¼Œæˆ‘ä»¬æå–å¥å­çº§åˆ«çš„ç‰¹å¾
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥åˆ‡å‰²éŸ³é¢‘segmentè¿›è¡Œåˆ†æ
        
        start_time = segment['start']
        end_time = segment['end']
        duration = end_time - start_time
        text = segment['text'].strip()
        
        # ç®€åŒ–çš„segmentçº§åˆ«ç‰¹å¾
        return {
            'prosody': {
                'duration': duration,
                'text_length': len(text),
                'prosody_flatness_score': None  # å¾…äººå·¥æ ‡æ³¨
            },
            'voice_quality': {
                'segment_duration': duration,
                'voice_hoarseness_score': None  # å¾…äººå·¥æ ‡æ³¨
            },
            'rhythm': {
                'articulation_rate': len(text) / duration if duration > 0 else 0,
                'speed_anomalies': []  # å¾…äººå·¥æ ‡æ³¨
            },
            'transcription': {
                'text': text,
                'filled_pauses_in_segment': self.count_filled_pauses_in_text(text),
                'repetitions_in_segment': self.count_repetitions_in_text(text)
            }
        }
    
    def extract_word_level_features(self, audio_path: str, segments: List[Dict]) -> Dict:
        """æå–è¯çº§åˆ«ç‰¹å¾ï¼ˆä¸»è¦æ˜¯å‘éŸ³æ¸…æ™°åº¦ï¼‰"""
        
        word_features = {}
        word_index = 0
        
        for segment in segments:
            words = segment['text'].strip().split()
            for word in words:
                if len(word) > 1:  # åªå¤„ç†æœ‰æ„ä¹‰çš„è¯
                    word_id = f"word_{word_index}"
                    word_features[word_id] = {
                        'articulation': {
                            'word': word,
                            'segment_start': segment['start'],
                            'segment_end': segment['end'],
                            'clarity_score': None,  # å¾…äººå·¥æ ‡æ³¨
                            'error_points': []  # å¾…äººå·¥æ ‡æ³¨ï¼š["SLUR", "SUB", "OMIT"]
                        }
                    }
                    word_index += 1
        
        return word_features
    
    def format_pause_details(self, pauses: List[Dict]) -> List[Dict]:
        """æ ¼å¼åŒ–åœé¡¿è¯¦ç»†ä¿¡æ¯"""
        
        formatted_pauses = []
        for i, pause in enumerate(pauses):
            pause_func = self.classify_pause_function(pause['duration'])
            
            formatted_pauses.append({
                'id': f"pause_{i}",
                'start': pause['start'],
                'end': pause['end'],
                'duration': pause['duration'],
                'position': "mid_sentence" if pause['duration'] > 1.0 else "sentence_boundary",
                'function': pause_func,  # åˆæ­¥åˆ†ç±»
                'confidence': 0.8  # åˆå§‹ç½®ä¿¡åº¦ï¼Œå¾…è®­ç»ƒæ¨¡å‹ä¼˜åŒ–
            })
        
        return formatted_pauses
    
    def count_filled_pauses_in_text(self, text: str) -> int:
        """ç»Ÿè®¡æ–‡æœ¬ä¸­çš„å¡«å……è¯"""
        filled_pauses = ['å—¯', 'å•Š', 'å‘ƒ', 'å‘', 'é‚£ä¸ª', 'å°±æ˜¯']
        count = 0
        for fp in filled_pauses:
            count += text.count(fp)
        return count
    
    def count_repetitions_in_text(self, text: str) -> int:
        """ç»Ÿè®¡æ–‡æœ¬ä¸­çš„é‡å¤"""
        words = text.split()
        word_counts = {}
        for word in words:
            if len(word) > 1:
                word_counts[word] = word_counts.get(word, 0) + 1
        return sum(1 for count in word_counts.values() if count > 1)

def generate_test_audio():
    """ç”Ÿæˆæµ‹è¯•ç”¨çš„éŸ³é¢‘æ–‡ä»¶"""
    
    logger.info("ğŸµ ç”Ÿæˆæµ‹è¯•éŸ³é¢‘æ–‡ä»¶...")
    
    # åˆ›å»ºæ ·æœ¬ç›®å½•
    sample_dir = Path("data/raw/audio/samples")
    sample_dir.mkdir(parents=True, exist_ok=True)
    
    try:
        import soundfile as sf
        
        # ç”Ÿæˆä¸€æ®µç®€å•çš„æµ‹è¯•è¯­éŸ³ï¼ˆæ­£å¼¦æ³¢ + å™ªå£°æ¨¡æ‹Ÿï¼‰
        duration = 5.0  # 5ç§’
        sample_rate = 16000
        t = np.linspace(0, duration, int(sample_rate * duration))
        
        # ç”Ÿæˆ3ä¸ªä¸åŒçš„æµ‹è¯•éŸ³é¢‘
        test_files = []
        
        for i in range(3):
            # åŸºç¡€é¢‘ç‡ï¼ˆæ¨¡æ‹Ÿä¸åŒçš„åŸºé¢‘ï¼‰
            f0 = 150 + i * 50  # 150Hz, 200Hz, 250Hz
            
            # ç”Ÿæˆå¤åˆä¿¡å·ï¼ˆæ¨¡æ‹Ÿè¯­éŸ³ï¼‰
            signal = (
                0.5 * np.sin(2 * np.pi * f0 * t) +           # åŸºé¢‘
                0.3 * np.sin(2 * np.pi * f0 * 2 * t) +       # äºŒæ¬¡è°æ³¢
                0.2 * np.sin(2 * np.pi * f0 * 3 * t) +       # ä¸‰æ¬¡è°æ³¢
                0.1 * np.random.normal(0, 0.1, len(t))       # å™ªå£°
            )
            
            # æ·»åŠ ä¸€äº›"åœé¡¿"ï¼ˆé™éŸ³æ®µï¼‰
            pause_start = int(sample_rate * 2)
            pause_end = int(sample_rate * 2.5)
            signal[pause_start:pause_end] = 0
            
            # å¦ä¸€ä¸ªåœé¡¿
            pause_start2 = int(sample_rate * 3.5)
            pause_end2 = int(sample_rate * 4)
            signal[pause_start2:pause_end2] = 0
            
            # å½’ä¸€åŒ–
            signal = signal / np.max(np.abs(signal)) * 0.8
            
            # ä¿å­˜æ–‡ä»¶
            file_path = sample_dir / f"test_audio_{i+1}.wav"
            sf.write(file_path, signal, sample_rate)
            test_files.append(file_path)
            
            logger.info(f"  âœ“ ç”Ÿæˆ: {file_path.name}")
        
        logger.info(f"âœ… ç”Ÿæˆäº† {len(test_files)} ä¸ªæµ‹è¯•éŸ³é¢‘æ–‡ä»¶")
        return test_files
        
    except ImportError:
        logger.warning("ç¼ºå°‘ soundfile åº“ï¼Œå°è¯•ä½¿ç”¨ scipy ç”ŸæˆéŸ³é¢‘...")
        
        try:
            from scipy.io import wavfile
            
            # ä½¿ç”¨scipyç”Ÿæˆæ›´ç®€å•çš„æµ‹è¯•éŸ³é¢‘
            duration = 3.0
            sample_rate = 16000
            t = np.linspace(0, duration, int(sample_rate * duration))
            
            test_files = []
            for i in range(2):
                # ç®€å•çš„æ­£å¼¦æ³¢
                f0 = 200 + i * 100
                signal = 0.5 * np.sin(2 * np.pi * f0 * t)
                
                # æ·»åŠ åœé¡¿
                pause_start = int(sample_rate * 1)
                pause_end = int(sample_rate * 1.5)
                signal[pause_start:pause_end] = 0
                
                # è½¬æ¢ä¸º16ä½æ•´æ•°
                signal_int = (signal * 32767).astype(np.int16)
                
                file_path = sample_dir / f"test_simple_{i+1}.wav"
                wavfile.write(file_path, sample_rate, signal_int)
                test_files.append(file_path)
                
                logger.info(f"  âœ“ ç”Ÿæˆ: {file_path.name}")
            
            logger.info(f"âœ… ç”Ÿæˆäº† {len(test_files)} ä¸ªç®€å•æµ‹è¯•éŸ³é¢‘æ–‡ä»¶")
            return test_files
            
        except Exception as e:
            logger.error(f"âŒ ç”ŸæˆéŸ³é¢‘æ–‡ä»¶å¤±è´¥: {e}")
            return []

def test_seven_dimension_extractor():
    """æµ‹è¯•7ç»´åº¦ç‰¹å¾æå–å™¨"""
    
    logger.info("ğŸš€ å¼€å§‹7ç»´åº¦å£°å­¦ç‰¹å¾æå–æµ‹è¯•...")
    
    # é¦–å…ˆæ£€æŸ¥SeniorTalkçœŸå®æ•°æ®
    seniortalk_dir = Path("data/processed/seniortalk_samples/audio")
    audio_files = list(seniortalk_dir.glob("*.wav")) if seniortalk_dir.exists() else []
    
    if audio_files:
        logger.info(f"âœ… æ‰¾åˆ°SeniorTalkçœŸå®æ•°æ®: {len(audio_files)} ä¸ªæ–‡ä»¶")
        
        # åŠ è½½æ•°æ®é›†å…ƒæ•°æ®
        metadata_file = Path("data/processed/seniortalk_samples/metadata/dataset_info.json")
        if metadata_file.exists():
            with open(metadata_file, 'r', encoding='utf-8') as f:
                import json
                metadata = json.load(f)
                logger.info(f"ğŸ“Š æ•°æ®é›†: {metadata['dataset_name']}")
                logger.info(f"ğŸ“ æ ·æœ¬æ•°: {metadata['total_samples']}")
    else:
        # å¤‡ç”¨æ–¹æ¡ˆï¼šæ£€æŸ¥æ ·æœ¬æ–‡ä»¶
        sample_dir = Path("data/raw/audio/samples")
        audio_files = list(sample_dir.rglob("*.wav"))
        
        if not audio_files:
            # ä¹Ÿæ£€æŸ¥å…¶ä»–éŸ³é¢‘æ ¼å¼
            audio_files = list(sample_dir.rglob("*.mp3")) + list(sample_dir.rglob("*.m4a"))
        
        if not audio_files:
            logger.warning("âŒ æœªæ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶ï¼Œå°è¯•ç”Ÿæˆæµ‹è¯•éŸ³é¢‘...")
            audio_files = generate_test_audio()
            
            if not audio_files:
                logger.error("âŒ æ— æ³•ç”Ÿæˆæµ‹è¯•éŸ³é¢‘æ–‡ä»¶")
                return False
    
    # åˆ›å»º7ç»´åº¦ç‰¹å¾æå–å™¨
    logger.info("ğŸ”§ åˆå§‹åŒ–7ç»´åº¦ç‰¹å¾æå–å™¨...")
    extractor = SevenDimensionAcousticExtractor()
    
    # æµ‹è¯•æ–‡ä»¶ - å¢åŠ æµ‹è¯•æ•°é‡
    num_files = min(3, len(audio_files))  # æµ‹è¯•å‰3ä¸ªæ–‡ä»¶
    for i, audio_file in enumerate(audio_files[:num_files]):
        logger.info(f"\n{'='*70}")
        logger.info(f"ğŸ¯ åˆ†æSeniorTalkæ ·æœ¬ {i+1}/{num_files}: {audio_file.name}")
        
        # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
        file_size = audio_file.stat().st_size / 1024  # KB
        logger.info(f"ğŸ“ æ–‡ä»¶å¤§å°: {file_size:.1f}KB")
        
        try:
            result = extractor.process_audio_7_dimensions(str(audio_file))
            
            if result is None:
                logger.error("âŒ å¤„ç†å¤±è´¥")
                continue
            
            # æ‰“å°è¯¦ç»†ç»“æœ
            logger.info("\nğŸ™ï¸ è€å¹´äººè¯­éŸ³7ç»´åº¦åˆ†æç»“æœ:")
            logger.info(f"ğŸ“ è¯†åˆ«æ–‡æœ¬: {result['text']}")
            logger.info(f"ğŸ—¨ï¸ åŸå§‹æ–‡æœ¬: {result['original_text']}")
            
            # æ˜¾ç¤ºç‰¹å¾æ˜ å°„ç»“æ„
            feature_map = result['acoustic_feature_map']
            
            # å¥å­çº§åˆ«ç‰¹å¾ï¼ˆæ˜¾ç¤ºç¬¬ä¸€ä¸ªsegmentï¼‰
            if 'segment_0' in feature_map:
                seg0 = feature_map['segment_0']
                
                # ç»´åº¦1: éŸµå¾‹
                prosody = seg0['prosody']
                logger.info(f"ğŸµ éŸµå¾‹ç‰¹å¾ (segment_0):")
                logger.info(f"  æ–‡æœ¬é•¿åº¦: {prosody['text_length']}")
                logger.info(f"  ç‰‡æ®µæ—¶é•¿: {prosody['duration']:.2f}s")
                
                # ç»´åº¦2: éŸ³è´¨
                voice = seg0['voice_quality']
                logger.info(f"ğŸ”Š éŸ³è´¨ç‰¹å¾ (segment_0):")
                logger.info(f"  ç‰‡æ®µæ—¶é•¿: {voice['segment_duration']:.2f}s")
                
                # ç»´åº¦4: è¯­é€ŸèŠ‚å¾‹
                rhythm = seg0['rhythm']
                logger.info(f"â±ï¸  è¯­é€ŸèŠ‚å¾‹ (segment_0):")
                logger.info(f"  å‘éŸ³é€Ÿç‡: {rhythm['articulation_rate']:.2f} å­—ç¬¦/ç§’")
                
                # ç»´åº¦5: æ–‡æœ¬ç‰¹å¾
                trans = seg0['transcription']
                logger.info(f"ğŸ“ æ–‡æœ¬ç‰¹å¾ (segment_0):")
                logger.info(f"  æ–‡æœ¬: {trans['text']}")
                logger.info(f"  å¡«å……è¯æ•°é‡: {trans['filled_pauses_in_segment']}")
                logger.info(f"  é‡å¤æ¬¡æ•°: {trans['repetitions_in_segment']}")
            
            # è¯çº§åˆ«ç‰¹å¾ï¼ˆæ˜¾ç¤ºç¬¬ä¸€ä¸ªwordï¼‰
            if 'word_0' in feature_map:
                word0 = feature_map['word_0']
                
                # ç»´åº¦3: å‘éŸ³æ¸…æ™°åº¦
                artic = word0['articulation']
                logger.info(f"ğŸ—£ï¸  å‘éŸ³æ¸…æ™°åº¦ (word_0):")
                logger.info(f"  è¯æ±‡: {artic['word']}")
                logger.info(f"  æ—¶é—´èŒƒå›´: {artic['segment_start']:.1f}s - {artic['segment_end']:.1f}s")
            
            # ç»´åº¦6: åœé¡¿åˆ†æ
            if 'global_pause_analysis' in feature_map:
                pause = feature_map['global_pause_analysis']
                logger.info(f"â¸ï¸  åœé¡¿åˆ†æ:")
                logger.info(f"  åœé¡¿æ¬¡æ•°: {pause['pause_statistics']['total_count']}")
                logger.info(f"  åœé¡¿æ¯”ä¾‹: {pause['pause_statistics']['pause_rate']:.1%}")
                
                if pause['pause_details']:
                    logger.info(f"  åœé¡¿è¯¦æƒ…: {len(pause['pause_details'])} ä¸ªåœé¡¿")
                    for p in pause['pause_details'][:2]:  # åªæ˜¾ç¤ºå‰2ä¸ª
                        logger.info(f"    {p['id']}: {p['duration']:.1f}s ({p['function']})")
            
            # æ˜¾ç¤ºç‰¹å¾æ˜ å°„æ¦‚è§ˆ
            logger.info(f"\nğŸ—‚ï¸  ç‰¹å¾æ˜ å°„æ¦‚è§ˆ:")
            for key in feature_map.keys():
                if key.startswith('segment_'):
                    logger.info(f"  {key}: å¥å­çº§ç‰¹å¾")
                elif key.startswith('word_'):
                    logger.info(f"  {key}: è¯çº§ç‰¹å¾")
                else:
                    logger.info(f"  {key}: å…¨å±€ç‰¹å¾")
            
            # æ€»ä½“è¯„ä¼°
            summary = result['summary']
            logger.info(f"\nğŸ“Š æ€»ä½“è¯„ä¼°:")
            logger.info(f"  éŸ³é¢‘æ—¶é•¿: {summary['total_duration']:.1f}ç§’")
            logger.info(f"  è¯­é€Ÿ: {summary['speech_rate']:.1f} å­—/åˆ†é’Ÿ")
            logger.info(f"  åœé¡¿æ¯”ä¾‹: {summary['pause_ratio']:.1%}")
            
            logger.info("âœ… 7ç»´åº¦ç‰¹å¾æå–æˆåŠŸ!")
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False
    
    return True

def main():
    """ä¸»å‡½æ•°"""
    
    if test_seven_dimension_extractor():
        logger.info("\nğŸ‰ 7ç»´åº¦å£°å­¦ç‰¹å¾æå–å™¨æµ‹è¯•é€šè¿‡ï¼")
        logger.info("\nğŸ¯ ç³»ç»ŸåŠŸèƒ½:")
        logger.info("  âœ“ 1. éŸµå¾‹å’Œè¯­è°ƒåˆ†æ (F0å˜åŒ–ã€è¯­è°ƒè½®å»“)")
        logger.info("  âœ“ 2. éŸ³è´¨å’Œç¨³å®šæ€§ (Jitterã€Shimmerã€HNR)")
        logger.info("  âœ“ 3. å‘éŸ³æ¸…æ™°åº¦ (VSAã€å…±æŒ¯å³°å˜åŒ–)")
        logger.info("  âœ“ 4. è¯­é€Ÿå’ŒèŠ‚å¾‹ (å‘éŸ³é€Ÿç‡ã€æ—¶é•¿å˜å¼‚)")
        logger.info("  âœ“ 5. æ–‡æœ¬å†…å®¹åˆ†æ (å¡«å……è¯ã€é‡å¤)")
        logger.info("  âœ“ 6. åœé¡¿ç»“æ„ä¸åŠŸèƒ½ (æ—¶é•¿ã€ä½ç½®ã€åŠŸèƒ½)")
        logger.info("  âœ“ 7. æ„éŸ³éšœç¢ç‚¹æ£€æµ‹ (é”™è¯¯ç±»å‹å®šä½)")
        logger.info("\nğŸ“‹ ä¸‹ä¸€æ­¥:")
        logger.info("  1. æ”¶é›†äººå·¥æ ‡æ³¨æ•°æ®è¿›è¡Œæ„ŸçŸ¥è¯„ä¼°")
        logger.info("  2. è®­ç»ƒé¢„æµ‹æ¨¡å‹ä¼˜åŒ–ç‰¹å¾æå–")
        logger.info("  3. å»ºç«‹MCIæ£€æµ‹çš„å®Œæ•´æµç¨‹")
    else:
        logger.error("âŒ 7ç»´åº¦ç‰¹å¾æå–å™¨æµ‹è¯•å¤±è´¥")

if __name__ == "__main__":
    main() 