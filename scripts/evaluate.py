#!/usr/bin/env python3
"""
æ¨¡å‹è¯„ä¼°è„šæœ¬
Model evaluation script for comprehensive assessment
"""

import os
import sys
import argparse
import logging
from pathlib import Path
import yaml
import torch
from torch.utils.data import DataLoader
import pandas as pd
import numpy as np
import json
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.models.integrated_model import IntegratedAlzheimerModel
from src.data.dataset import AlzheimerDataset
from src.utils.metrics import compute_classification_metrics, compute_concept_metrics
from src.utils.visualization import plot_confusion_matrix, plot_concept_predictions

logger = logging.getLogger(__name__)


class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self, model_path: str, config_path: str):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åŠ è½½é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        # åŠ è½½æ¨¡å‹
        logger.info(f"ä» {model_path} åŠ è½½æ¨¡å‹...")
        checkpoint = torch.load(model_path, map_location=self.device)
        
        self.model = IntegratedAlzheimerModel(checkpoint['config']['model'])
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.to(self.device)
        self.model.eval()
        
        # ç±»åˆ«åç§°
        self.class_names = checkpoint['config']['model']['crf']['class_names']
        
        logger.info("æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def evaluate_dataset(
        self, 
        dataloader: DataLoader, 
        return_predictions: bool = True
    ) -> dict:
        """è¯„ä¼°æ•°æ®é›†"""
        
        all_predictions = []
        all_targets = []
        all_concept_predictions = {}
        all_concept_targets = {}
        all_probabilities = []
        sample_results = []
        
        logger.info("å¼€å§‹è¯„ä¼°...")
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(dataloader):
                batch = {k: v.to(self.device) if torch.is_tensor(v) else v 
                        for k, v in batch.items()}
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(batch, return_concepts=True, return_explanations=True)
                
                # æ”¶é›†åˆ†ç±»ç»“æœ
                predictions = outputs['diagnosis_predictions'].cpu().numpy()
                probabilities = outputs['diagnosis_probs'].cpu().numpy()
                targets = batch['diagnosis'].cpu().numpy()
                
                all_predictions.extend(predictions)
                all_targets.extend(targets)
                all_probabilities.extend(probabilities)
                
                # æ”¶é›†æ¦‚å¿µé¢„æµ‹
                for concept_name, concept_pred in outputs['concepts'].items():
                    if concept_name not in all_concept_predictions:
                        all_concept_predictions[concept_name] = []
                        all_concept_targets[concept_name] = []
                    
                    all_concept_predictions[concept_name].extend(concept_pred.cpu().numpy())
                    
                    if 'concepts' in batch and concept_name in batch['concepts']:
                        all_concept_targets[concept_name].extend(
                            batch['concepts'][concept_name].cpu().numpy()
                        )
                
                # æ”¶é›†æ ·æœ¬çº§ç»“æœï¼ˆç”¨äºè¯¦ç»†åˆ†æï¼‰
                if return_predictions:
                    batch_size = len(predictions)
                    for i in range(batch_size):
                        sample_result = {
                            'prediction': int(predictions[i]),
                            'target': int(targets[i]),
                            'probabilities': probabilities[i].tolist(),
                            'concepts': {name: float(values[i]) 
                                       for name, values in outputs['concepts'].items()}
                        }
                        
                        if 'sample_id' in batch:
                            sample_result['sample_id'] = batch['sample_id'][i]
                        else:
                            sample_result['sample_id'] = batch_idx * dataloader.batch_size + i
                        
                        sample_results.append(sample_result)
        
        # è®¡ç®—åˆ†ç±»æŒ‡æ ‡
        classification_metrics = compute_classification_metrics(
            all_targets, all_predictions, self.class_names
        )
        
        # è®¡ç®—æ¦‚å¿µæŒ‡æ ‡
        concept_metrics = {}
        if all_concept_targets:
            concept_metrics = compute_concept_metrics(
                all_concept_targets, all_concept_predictions
            )
        
        # æ„å»ºç»“æœå­—å…¸
        results = {
            'classification_metrics': classification_metrics,
            'concept_metrics': concept_metrics,
            'predictions': all_predictions,
            'targets': all_targets,
            'probabilities': all_probabilities,
            'concept_predictions': all_concept_predictions,
            'concept_targets': all_concept_targets,
            'sample_results': sample_results if return_predictions else None
        }
        
        return results
    
    def generate_evaluation_report(
        self, 
        results: dict, 
        output_dir: Path,
        split_name: str = "test"
    ):
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # 1. ä¿å­˜æŒ‡æ ‡JSON
        metrics_file = output_dir / f"{split_name}_metrics.json"
        metrics_summary = {
            'classification_metrics': results['classification_metrics'],
            'concept_metrics': results['concept_metrics']
        }
        
        with open(metrics_file, 'w', encoding='utf-8') as f:
            json.dump(metrics_summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æŒ‡æ ‡å·²ä¿å­˜åˆ°: {metrics_file}")
        
        # 2. ç”Ÿæˆæ··æ·†çŸ©é˜µ
        cm_file = output_dir / f"{split_name}_confusion_matrix.png"
        plot_confusion_matrix(
            results['targets'], 
            results['predictions'], 
            self.class_names,
            save_path=str(cm_file)
        )
        
        # 3. ç”Ÿæˆæ¦‚å¿µé¢„æµ‹å›¾ï¼ˆå¦‚æœæœ‰æ¦‚å¿µæ ‡ç­¾ï¼‰
        if results['concept_targets']:
            concept_plot_file = output_dir / f"{split_name}_concept_predictions.png"
            plot_concept_predictions(
                results['concept_targets'],
                results['concept_predictions'],
                save_path=str(concept_plot_file)
            )
        
        # 4. ç”Ÿæˆè¯¦ç»†åˆ†ç±»æŠ¥å‘Š
        report_file = output_dir / f"{split_name}_classification_report.txt"
        report = classification_report(
            results['targets'], 
            results['predictions'], 
            target_names=self.class_names,
            digits=4
        )
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("åˆ†ç±»æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            f.write(report)
            f.write("\n\n")
            f.write("è¯¦ç»†æŒ‡æ ‡\n")
            f.write("-" * 30 + "\n")
            for metric, value in results['classification_metrics'].items():
                f.write(f"{metric}: {value:.4f}\n")
        
        # 5. æ¦‚å¿µæŒ‡æ ‡æŠ¥å‘Š
        if results['concept_metrics']:
            concept_report_file = output_dir / f"{split_name}_concept_report.txt"
            with open(concept_report_file, 'w', encoding='utf-8') as f:
                f.write("æ¦‚å¿µé¢„æµ‹æŠ¥å‘Š\n")
                f.write("=" * 50 + "\n\n")
                
                for concept_name, metrics in results['concept_metrics'].items():
                    f.write(f"{concept_name}:\n")
                    for metric_name, value in metrics.items():
                        f.write(f"  {metric_name}: {value:.4f}\n")
                    f.write("\n")
        
        # 6. ä¿å­˜æ ·æœ¬çº§é¢„æµ‹ç»“æœ
        if results['sample_results']:
            samples_file = output_dir / f"{split_name}_sample_predictions.json"
            with open(samples_file, 'w', encoding='utf-8') as f:
                json.dump(results['sample_results'], f, ensure_ascii=False, indent=2)
        
        # 7. ç”ŸæˆHTMLæŠ¥å‘Š
        self._generate_html_report(results, output_dir / f"{split_name}_report.html", split_name)
        
        logger.info(f"è¯„ä¼°æŠ¥å‘Šå·²ç”Ÿæˆåˆ°: {output_dir}")
    
    def _generate_html_report(self, results: dict, output_file: Path, split_name: str):
        """ç”ŸæˆHTMLè¯„ä¼°æŠ¥å‘Š"""
        
        html_content = f"""
        <!DOCTYPE html>
        <html lang="zh-CN">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>{split_name.upper()} æ•°æ®é›†è¯„ä¼°æŠ¥å‘Š</title>
            <style>
                body {{ font-family: 'Microsoft YaHei', Arial, sans-serif; margin: 20px; }}
                .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 10px; }}
                .section {{ background: #f8f9fa; margin: 15px 0; padding: 15px; border-radius: 8px; }}
                .metric {{ background: white; margin: 5px 0; padding: 10px; border-radius: 5px; border-left: 4px solid #007bff; }}
                .metric-value {{ font-weight: bold; color: #28a745; }}
                table {{ width: 100%; border-collapse: collapse; margin: 10px 0; }}
                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: center; }}
                th {{ background-color: #f2f2f2; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>ğŸ§  é˜¿å°”èŒ¨æµ·é»˜ç—‡æ£€æµ‹æ¨¡å‹è¯„ä¼°æŠ¥å‘Š</h1>
                <p>æ•°æ®é›†: {split_name.upper()}</p>
                <p>è¯„ä¼°æ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
            </div>
            
            <div class="section">
                <h2>ğŸ“Š åˆ†ç±»æ€§èƒ½æŒ‡æ ‡</h2>
        """
        
        # æ·»åŠ åˆ†ç±»æŒ‡æ ‡
        for metric, value in results['classification_metrics'].items():
            if not metric.endswith('_Healthy') and not metric.endswith('_MCI') and not metric.endswith('_AD'):
                html_content += f"""
                <div class="metric">
                    <span>{metric}:</span> <span class="metric-value">{value:.4f}</span>
                </div>
                """
        
        # æ·»åŠ æ¯ç±»åˆ«æŒ‡æ ‡è¡¨æ ¼
        html_content += """
                <h3>å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡</h3>
                <table>
                    <tr><th>ç±»åˆ«</th><th>ç²¾ç¡®ç‡</th><th>å¬å›ç‡</th><th>F1åˆ†æ•°</th></tr>
        """
        
        for class_name in self.class_names:
            precision = results['classification_metrics'].get(f'precision_{class_name}', 0)
            recall = results['classification_metrics'].get(f'recall_{class_name}', 0)
            f1 = results['classification_metrics'].get(f'f1_{class_name}', 0)
            
            html_content += f"""
                    <tr>
                        <td>{class_name}</td>
                        <td>{precision:.4f}</td>
                        <td>{recall:.4f}</td>
                        <td>{f1:.4f}</td>
                    </tr>
            """
        
        html_content += """
                </table>
            </div>
        """
        
        # æ·»åŠ æ¦‚å¿µæŒ‡æ ‡ï¼ˆå¦‚æœæœ‰ï¼‰
        if results['concept_metrics']:
            html_content += """
            <div class="section">
                <h2>ğŸ” æ¦‚å¿µé¢„æµ‹æ€§èƒ½</h2>
                <table>
                    <tr><th>æ¦‚å¿µ</th><th>MAE</th><th>RMSE</th><th>RÂ²</th><th>ç›¸å…³ç³»æ•°</th></tr>
            """
            
            for concept_name, metrics in results['concept_metrics'].items():
                html_content += f"""
                    <tr>
                        <td>{concept_name}</td>
                        <td>{metrics.get('mae', 0):.4f}</td>
                        <td>{metrics.get('rmse', 0):.4f}</td>
                        <td>{metrics.get('r2', 0):.4f}</td>
                        <td>{metrics.get('correlation', 0):.4f}</td>
                    </tr>
                """
            
            html_content += """
                </table>
            </div>
            """
        
        html_content += """
            <div class="section">
                <h2>ğŸ“ˆ å¯è§†åŒ–å›¾è¡¨</h2>
                <p>æ··æ·†çŸ©é˜µå’Œæ¦‚å¿µé¢„æµ‹å›¾è¡¨å·²ä¿å­˜ä¸ºPNGæ–‡ä»¶ï¼Œè¯·æŸ¥çœ‹ç›¸å…³æ–‡ä»¶ã€‚</p>
            </div>
            
            <div class="section">
                <h2>âš ï¸ è¯´æ˜</h2>
                <p>æœ¬æŠ¥å‘Šå±•ç¤ºäº†æ¨¡å‹åœ¨æŒ‡å®šæ•°æ®é›†ä¸Šçš„æ€§èƒ½è¡¨ç°ã€‚è¯·ç»“åˆé¢†åŸŸçŸ¥è¯†å’Œä¸´åºŠç»éªŒè§£é‡Šç»“æœã€‚</p>
            </div>
        </body>
        </html>
        """
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html_content)


def main():
    parser = argparse.ArgumentParser(description="é˜¿å°”èŒ¨æµ·é»˜ç—‡æ£€æµ‹æ¨¡å‹è¯„ä¼°")
    parser.add_argument('--model_path', type=str, required=True, help='æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--config_path', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--data_path', type=str, required=True, help='æµ‹è¯•æ•°æ®è·¯å¾„')
    parser.add_argument('--output_dir', type=str, required=True, help='è¾“å‡ºç›®å½•')
    parser.add_argument('--split_name', type=str, default='test', help='æ•°æ®é›†åˆ†å‰²åç§°')
    parser.add_argument('--batch_size', type=int, default=32, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--num_workers', type=int, default=4, help='æ•°æ®åŠ è½½è¿›ç¨‹æ•°')
    parser.add_argument('--debug', action='store_true', help='è°ƒè¯•æ¨¡å¼')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.DEBUG if args.debug else logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # ç¡®å®šé…ç½®æ–‡ä»¶è·¯å¾„
    if args.config_path:
        config_path = args.config_path
    else:
        # å°è¯•ä»æ¨¡å‹ç›®å½•æ‰¾é…ç½®æ–‡ä»¶
        model_dir = Path(args.model_path).parent.parent
        config_path = model_dir / "config" / "model_config.yaml"
        if not config_path.exists():
            config_path = "config/model_config.yaml"
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = ModelEvaluator(args.model_path, config_path)
    
    # åŠ è½½æ•°æ®
    logger.info(f"åŠ è½½æ•°æ®: {args.data_path}")
    dataset = AlzheimerDataset(
        args.data_path,
        config=evaluator.config.get('data', {}),
        mode='test'
    )
    
    dataloader = DataLoader(
        dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=args.num_workers,
        pin_memory=True
    )
    
    # è¿è¡Œè¯„ä¼°
    logger.info("å¼€å§‹æ¨¡å‹è¯„ä¼°...")
    results = evaluator.evaluate_dataset(dataloader, return_predictions=True)
    
    # ç”ŸæˆæŠ¥å‘Š
    output_dir = Path(args.output_dir)
    evaluator.generate_evaluation_report(results, output_dir, args.split_name)
    
    # æ‰“å°å…³é”®æŒ‡æ ‡
    print("\n" + "="*60)
    print(f"{args.split_name.upper()} æ•°æ®é›†è¯„ä¼°ç»“æœ")
    print("="*60)
    print(f"å‡†ç¡®ç‡: {results['classification_metrics']['accuracy']:.4f}")
    print(f"å®å¹³å‡F1: {results['classification_metrics']['f1_macro']:.4f}")
    print(f"å¾®å¹³å‡F1: {results['classification_metrics']['f1_micro']:.4f}")
    
    if results['concept_metrics']:
        print("\næ¦‚å¿µé¢„æµ‹æ€§èƒ½:")
        for concept_name, metrics in results['concept_metrics'].items():
            print(f"  {concept_name}: MAE={metrics.get('mae', 0):.4f}, RÂ²={metrics.get('r2', 0):.4f}")
    
    print("="*60)
    logger.info("è¯„ä¼°å®Œæˆ!")


if __name__ == "__main__":
    import pandas as pd  # ç”¨äºæ—¶é—´æˆ³
    main() 